{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3341dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pickle import dump, load\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "479cf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.years     = [2013, 2014, 2015, 2016, 2017, 2018]\n",
    "        self.suppliers = ['A', 'B', 'C', 'D', 'E', 'G', 'H']\n",
    "        self.datas     = self.load_data()\n",
    "        self.month_nums  = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "    \n",
    "    def load_data(self):\n",
    "        datas = dict()\n",
    "        for supplier in suppliers:\n",
    "            datas['data_{}'.format(supplier)] = pd.read_csv('./Data/supplier_{}.csv'.format(supplier)) \n",
    "        \n",
    "        return datas\n",
    "    \n",
    "    def get_december_data(self, datas):\n",
    "        # getting the last day of december (31st) of each year except 2018\n",
    "        \n",
    "        training_12_all = []\n",
    "        for key in datas.keys():\n",
    "            training_12  = datas[key][ (datas[key]['month'] == 12) & \\\n",
    "                                      (datas[key]['year'].isin([2013, 2014, 2015,2016, 2017])) ].reset_index(drop = True)\n",
    "\n",
    "\n",
    "            ### for cutting out all december days except the 31st\n",
    "            training_12 = training_12.sort_values(['year', 'month','day']).reset_index(drop=True)\n",
    "\n",
    "            #print(training_12['year'].value_counts())\n",
    "            training_12_cut = []\n",
    "            for i in range(1,6):\n",
    "                training_12_cut.append(training_12.iloc[(744*i)-24:744*i])\n",
    "\n",
    "            training_12_final = pd.concat(training_12_cut, axis = 0).reset_index(drop=True)\n",
    "            #print(training_12_final['year'].value_counts())\n",
    "            \n",
    "            training_12_all.append(training_12_final)\n",
    "            \n",
    "        \n",
    "        return training_12_all\n",
    "    \n",
    "    def get_jan_march_data(self, datas):\n",
    "        \n",
    "        training_1_3_all = []\n",
    "        \n",
    "        for key in datas.keys():  \n",
    "            training_1_3 = datas[key][ (datas[key]['month'].isin([1,2,3])) & \\\n",
    "                             (datas[key]['year'].isin([2014, 2015, 2016, 2017, 2018])) ].reset_index(drop = True)\n",
    "            \n",
    "            training_1_3_all.append(training_1_3)\n",
    "        \n",
    "        return training_1_3_all\n",
    "    \n",
    "    def combine_data(self, training_12_all, training_1_3_all):\n",
    "        \n",
    "        combined_all = []\n",
    "        \n",
    "        assert len(training_12_all) == len(training_1_3_all), \"december and jan-march data lengths should match\"\n",
    "        \n",
    "        for i in range(len(training_12_all)):\n",
    "            training = pd.concat([training_12_all[i], training_1_3_all[i]], axis = 0).reset_index(drop=True)\n",
    "            training = training.sort_values(['year', 'month']).reset_index(drop=True)\n",
    "            \n",
    "            combined_all.append(training)\n",
    "        \n",
    "        return combined_all\n",
    "    \n",
    "    def separate_data(self, combined_data):\n",
    "        # separate 12/31 - 3/31 for each pair of year (e.g. 2013-2014, 2014-2015, etc.)\n",
    "        indices = []\n",
    "        for year in self.years[1:]: # march of 2014-2018\n",
    "            indices.append( combined_data[ (combined_data['year'] == year) & (combined_data['month'] == 3) &\\\n",
    "                           (combined_data['day'] == 31) & (combined_data['시간'] == 24)].index[0] )\n",
    "        print(indices)\n",
    "        training_sep = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            idx = idx + 1 \n",
    "            #if idx != training_A.shape[0] -1 else idx\n",
    "            training_sep.append(combined_data[last_idx:idx][['공급량', 'year', 'month', 'day', '시간']])\n",
    "            last_idx = idx\n",
    "        \n",
    "        return training_sep\n",
    "    \n",
    "    \n",
    "    def make_scalers(self, combined_all):\n",
    "        combined = combined_all[:]\n",
    "        # change pandas dataframe to numpy ndarray\n",
    "        for i in range(len(combined_all)): # loop through suppliers \n",
    "            supplier = combined[i]['구분'].iloc[0]\n",
    "            assert supplier == self.suppliers[i], 'suppliers should match'\n",
    "            combined[i] = combined[i][['공급량', 'year', 'month', 'day', '시간']].values\n",
    "            #print(combined[i].shape)\n",
    "            x_scaler = MinMaxScaler()\n",
    "            y_scaler = MinMaxScaler()\n",
    "            x_scaler.fit(combined[i])\n",
    "            y_scaler.fit(combined[i][:, [0]])\n",
    "            \n",
    "            path_x = './Scalers/x_scaler_{}.pkl'.format(supplier)\n",
    "            dump(x_scaler, open(path_x, 'wb'))\n",
    "            path_y = './Scalers/y_scaler_{}.pkl'.format(supplier)\n",
    "            dump(y_scaler, open(path_y, 'wb'))\n",
    "            \n",
    "        \n",
    "    \n",
    "    def create_LSTM_Input(self, data, seq_length, step_size):\n",
    "        final_data = np.zeros((data.shape[0]-seq_length, int(seq_length/step_size), data.shape[1]))\n",
    "\n",
    "        length = data.shape[0]\n",
    "\n",
    "        for i in range(final_data.shape[0]):\n",
    "            final_data[i] = data[i:i+seq_length:step_size]\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    def create_LSTM_Output(self, data, seq_length):\n",
    "        final_output = []\n",
    "        length = data.shape[0]\n",
    "\n",
    "        for i in range(seq_length, length):\n",
    "            final_output.append(data[i])\n",
    "\n",
    "        final_output = np.array(final_output)\n",
    "        #final_output = np.expand_dims(final_output, axis = 1)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    \n",
    "    def separate_data_to_lstm_input(self, sep_data, x_scaler, y_scaler, seq_length): \n",
    "        # need to make each chunck 12-3 into lstm inputs SEPARATELY\n",
    "        lstm_input_lst = []\n",
    "        lstm_output_lst = []\n",
    "        for i in range(len(sep_data)):\n",
    "            sep_values = sep_data[i].values\n",
    "            x_norm = x_scaler.transform(sep_values)\n",
    "            #[:,[0, 4]]\n",
    "            y_norm = y_scaler.transform(sep_values[:, [0]])\n",
    "            lstm_input_lst.append(self.create_LSTM_Input(x_norm, seq_length, 1))\n",
    "            lstm_output_lst.append(self.create_LSTM_Output(y_norm, seq_length))\n",
    "\n",
    "\n",
    "        lstm_input = np.concatenate(lstm_input_lst, axis = 0)\n",
    "        lstm_output = np.concatenate(lstm_output_lst, axis = 0)\n",
    "        \n",
    "        return lstm_input, lstm_output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "41351dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/woojaebyun/Documents/Dacon_Gas_Prediction'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047613f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "72afa374",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = data_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0eb6103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.901489e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        1.000000e+00],\n",
       "       [1.706081e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        2.000000e+00],\n",
       "       [1.533921e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        3.000000e+00],\n",
       "       [1.611033e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        4.000000e+00],\n",
       "       [1.792161e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        5.000000e+00]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_data = dp.get_december_data(dp.datas)\n",
    "jan_march_data = dp.get_jan_march_data(dp.datas)\n",
    "\n",
    "combined_all = dp.combine_data(dec_data, jan_march_data)\n",
    "\n",
    "combined_all[0][['공급량', 'year', 'month', 'day', '시간']].values[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "252f8e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2183, 4367, 6575, 8759, 10943]\n",
      "   Unnamed: 0         연월일  시간 구분       공급량  year  month  day\n",
      "0        8736  2013-12-31   1  B  1646.773  2013     12   31\n",
      "1        8737  2013-12-31   2  B  1412.293  2013     12   31\n",
      "2        8738  2013-12-31   3  B  1257.413  2013     12   31\n",
      "3        8739  2013-12-31   4  B  1293.573  2013     12   31\n",
      "4        8740  2013-12-31   5  B  1471.893  2013     12   31\n",
      "(10929, 3, 5)\n",
      "(10929, 1)\n"
     ]
    }
   ],
   "source": [
    "sep_data = dp.separate_data(combined_all[1])\n",
    "print(combined_all[1].iloc[:5])\n",
    "\n",
    "x_scaler = load(open('./Scalers/x_scaler_B.pkl', 'rb'))\n",
    "y_scaler = load(open('./Scalers/y_scaler_B.pkl', 'rb'))\n",
    "lstm_input, lstm_output = dp.separate_data_to_lstm_input(sep_data, x_scaler, y_scaler, 3)\n",
    "\n",
    "print(lstm_input.shape)\n",
    "print(lstm_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7bc3ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(lstm_input, lstm_output, test_size = 0.2, random_state = 1311, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "09a34493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, Dense, Input, LSTM, Flatten, Concatenate, Bidirectional, BatchNormalization, Dropout, ReLU, Activation, ConvLSTM2D, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f3be9478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 3, 5)]            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 3, 20)             2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 20)             80        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 3, 20)             0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 3, 10)             1240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 3, 10)             40        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 10)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                992       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,593\n",
      "Trainable params: 4,469\n",
      "Non-trainable params: 124\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_x = Input(shape=(3,5)) #(sequence length, num of features) for LSTM; i.e. 50 minutes (sequence of 50 minutes)\n",
    "\n",
    "x = LSTM(20, return_sequences=True)(input_x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "x = LSTM(10, return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "output = Dense(1)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs = input_x, outputs = output)\n",
    "model.compile(loss=\"mean_absolute_error\", optimizer = Adam(lr=0.001)) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8666b883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:28:52.233731: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:28:52.432208: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/69 [>.............................] - ETA: 1s - loss: 0.5445  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:28:52.464410: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:28:52.517818: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:28:52.567655: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - ETA: 0s - loss: 0.2150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:28:54.495363: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:28:54.560682: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:28:54.584039: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 3s 32ms/step - loss: 0.2150 - val_loss: 0.5613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56127, saving model to ./Models/training_A/Epoch_001_Val_0.561.hdf5\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.1160 - val_loss: 0.5627\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.56127\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.1004 - val_loss: 0.5256\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56127 to 0.52564, saving model to ./Models/training_A/Epoch_003_Val_0.526.hdf5\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0922 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52564 to 0.40091, saving model to ./Models/training_A/Epoch_004_Val_0.401.hdf5\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0855 - val_loss: 0.3048\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.40091 to 0.30479, saving model to ./Models/training_A/Epoch_005_Val_0.305.hdf5\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0838 - val_loss: 0.1942\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30479 to 0.19421, saving model to ./Models/training_A/Epoch_006_Val_0.194.hdf5\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0800 - val_loss: 0.1047\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19421 to 0.10467, saving model to ./Models/training_A/Epoch_007_Val_0.105.hdf5\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0770 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10467 to 0.08503, saving model to ./Models/training_A/Epoch_008_Val_0.085.hdf5\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0731 - val_loss: 0.0766\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08503 to 0.07663, saving model to ./Models/training_A/Epoch_009_Val_0.077.hdf5\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0726 - val_loss: 0.0741\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.07663 to 0.07412, saving model to ./Models/training_A/Epoch_010_Val_0.074.hdf5\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0715 - val_loss: 0.1531\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07412\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0734 - val_loss: 0.2197\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07412\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0781 - val_loss: 0.2398\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07412\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0721 - val_loss: 0.1646\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07412\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0700 - val_loss: 0.1121\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.07412\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0671 - val_loss: 0.0796\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.07412\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0657 - val_loss: 0.0690\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07412 to 0.06900, saving model to ./Models/training_A/Epoch_017_Val_0.069.hdf5\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0638 - val_loss: 0.0681\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06900 to 0.06805, saving model to ./Models/training_A/Epoch_018_Val_0.068.hdf5\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0631 - val_loss: 0.0606\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06805 to 0.06060, saving model to ./Models/training_A/Epoch_019_Val_0.061.hdf5\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0612 - val_loss: 0.1891\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06060\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0608 - val_loss: 0.1368\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06060\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0591 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06060\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0586 - val_loss: 0.0713\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06060\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0556 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.06060\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0555 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06060\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0534 - val_loss: 0.0619\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.06060\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0560 - val_loss: 0.1959\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.06060\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0587 - val_loss: 0.2993\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.06060\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0559 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.06060\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode = 'min' , patience = 10, verbose = 1)\n",
    "\n",
    "path = './Models/training_{}'.format('')\n",
    "if os.path.isdir(path):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "os.makedirs(path, exist_ok = True)\n",
    "\n",
    "file_path = path + '/Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5'\n",
    "mc = ModelCheckpoint(file_path, monitor='val_loss', mode='min',verbose=1, \\\n",
    "                     save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit(train_x, train_y, batch_size = 128, epochs =30, validation_data = (valid_x, valid_y), callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "04eec78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ace50400>]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607c92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
