{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "12e12cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pickle import dump, load\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "41429e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.years     = [2013, 2014, 2015, 2016, 2017, 2018]\n",
    "        self.suppliers = ['A', 'B', 'C', 'D', 'E', 'G', 'H']\n",
    "        self.datas     = self.load_data()\n",
    "        self.month_nums  = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        self.test_data = pd.read_csv('./Submission/test.csv')\n",
    "        self.model_paths = [ './Models/training_A/Epoch_027_Val_0.055.hdf5', \\\n",
    "                             './Models/training_B/Epoch_023_Val_0.046.hdf5', \\\n",
    "                             './Models/training_C/Epoch_017_Val_0.056.hdf5', \\\n",
    "                             './Models/training_D/Epoch_019_Val_0.047.hdf5', \\\n",
    "                             './Models/training_E/Epoch_017_Val_0.066.hdf5', \\\n",
    "                             './Models/training_G/Epoch_018_Val_0.056.hdf5', \\\n",
    "                             './Models/training_H/Epoch_029_Val_0.014.hdf5']\n",
    "        \n",
    "    \n",
    "    def load_data(self):\n",
    "        datas = dict()\n",
    "        for supplier in suppliers:\n",
    "            datas['data_{}'.format(supplier)] = pd.read_csv('./Data/supplier_{}.csv'.format(supplier)) \n",
    "        \n",
    "        return datas\n",
    "    \n",
    "    def get_december_data(self, datas):\n",
    "        # getting the last day of december (31st) of each year except 2018 for each supplier and return all in a list\n",
    "        # returns a list of december 31st data for all suppliers\n",
    "        # each element is december 31st data of each supplier\n",
    "        training_12_all = []\n",
    "        for key in datas.keys():\n",
    "            training_12  = datas[key][ (datas[key]['month'] == 12) & \\\n",
    "                                      (datas[key]['year'].isin([2013, 2014, 2015,2016, 2017])) ].reset_index(drop = True)\n",
    "\n",
    "\n",
    "            ### for cutting out all december days except the 31st\n",
    "            training_12 = training_12.sort_values(['year', 'month','day']).reset_index(drop=True)\n",
    "\n",
    "            #print(training_12['year'].value_counts())\n",
    "            training_12_cut = []\n",
    "            for i in range(1,6):\n",
    "                training_12_cut.append(training_12.iloc[(744*i)-24:744*i])\n",
    "\n",
    "            training_12_final = pd.concat(training_12_cut, axis = 0).reset_index(drop=True)\n",
    "            #print(training_12_final['year'].value_counts())\n",
    "            \n",
    "            training_12_all.append(training_12_final)\n",
    "            \n",
    "        \n",
    "        return training_12_all\n",
    "    \n",
    "    def get_jan_march_data(self, datas):\n",
    "        # get jan-march for each supplier and return all in a list\n",
    "        # returns a list of jan-march data for all suppliers \n",
    "        # each element is jan-march data of each supplier\n",
    "        training_1_3_all = []\n",
    "        \n",
    "        for key in datas.keys():  \n",
    "            training_1_3 = datas[key][ (datas[key]['month'].isin([1,2,3])) & \\\n",
    "                             (datas[key]['year'].isin([2014, 2015, 2016, 2017, 2018])) ].reset_index(drop = True)\n",
    "            \n",
    "            training_1_3_all.append(training_1_3)\n",
    "        \n",
    "        return training_1_3_all\n",
    "    \n",
    "    def combine_data(self, training_12_all, training_1_3_all):\n",
    "        # combine decebmer and jan-march data for each supplier\n",
    "        # returns a list of december + jan-march data for all suppliers\n",
    "        # each element is dec + jan-march data for each supplier\n",
    "        combined_all = []\n",
    "        \n",
    "        assert len(training_12_all) == len(training_1_3_all), \"december and jan-march data lengths should match\"\n",
    "        \n",
    "        for i in range(len(training_12_all)):\n",
    "            training = pd.concat([training_12_all[i], training_1_3_all[i]], axis = 0).reset_index(drop=True)\n",
    "            training = training.sort_values(['year', 'month']).reset_index(drop=True)\n",
    "            \n",
    "            combined_all.append(training)\n",
    "        \n",
    "        return combined_all\n",
    "    \n",
    "    def separate_data(self, combined_data):\n",
    "        # separate 12/31 - 3/31 for each pair of year (e.g. 2013-2014, 2014-2015, etc.)\n",
    "        # so that each pair can be made into lstm input and output \n",
    "        # this is important as we don't want dec 31st to be the y-label for march data\n",
    "        \n",
    "        indices = []\n",
    "        for year in self.years[1:]: # march of 2014-2018\n",
    "            indices.append( combined_data[ (combined_data['year'] == year) & (combined_data['month'] == 3) &\\\n",
    "                           (combined_data['day'] == 31) & (combined_data['시간'] == 24)].index[0] )\n",
    "        print(indices)\n",
    "        training_sep = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            idx = idx + 1 \n",
    "            #if idx != training_A.shape[0] -1 else idx\n",
    "            training_sep.append(combined_data[last_idx:idx][['공급량', 'year', 'month', 'day', '시간']])\n",
    "            last_idx = idx\n",
    "        \n",
    "        return training_sep\n",
    "    \n",
    "    \n",
    "    def make_scalers(self, combined_all):\n",
    "        combined = combined_all[:]\n",
    "        # change pandas dataframe to numpy ndarray\n",
    "        for i in range(len(combined_all)): # loop through suppliers \n",
    "            supplier = combined[i]['구분'].iloc[0]\n",
    "            assert supplier == self.suppliers[i], 'suppliers should match'\n",
    "            combined[i] = combined[i][['공급량', 'year', 'month', 'day', '시간']].values\n",
    "            #print(combined[i].shape)\n",
    "            x_scaler = MinMaxScaler()\n",
    "            y_scaler = MinMaxScaler()\n",
    "            x_scaler.fit(combined[i])\n",
    "            y_scaler.fit(combined[i][:, [0]])\n",
    "            \n",
    "            path_x = './Scalers/x_scaler_{}.pkl'.format(supplier)\n",
    "            dump(x_scaler, open(path_x, 'wb'))\n",
    "            path_y = './Scalers/y_scaler_{}.pkl'.format(supplier)\n",
    "            dump(y_scaler, open(path_y, 'wb'))\n",
    "            \n",
    "        \n",
    "    \n",
    "    def create_LSTM_Input(self, data, seq_length, step_size):\n",
    "        final_data = np.zeros((data.shape[0]-seq_length, int(seq_length/step_size), data.shape[1]))\n",
    "\n",
    "        length = data.shape[0]\n",
    "\n",
    "        for i in range(final_data.shape[0]):\n",
    "            final_data[i] = data[i:i+seq_length:step_size]\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    def create_LSTM_Output(self, data, seq_length):\n",
    "        final_output = []\n",
    "        length = data.shape[0]\n",
    "\n",
    "        for i in range(seq_length, length):\n",
    "            final_output.append(data[i])\n",
    "\n",
    "        final_output = np.array(final_output)\n",
    "        #final_output = np.expand_dims(final_output, axis = 1)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    \n",
    "    def separate_data_to_lstm_input(self, sep_data, x_scaler, y_scaler, seq_length): \n",
    "        # need to make each chunck 12-3 into lstm inputs SEPARATELY\n",
    "        # after separating dec-march pairs, we make them into lstm input and output\n",
    "        # returns all lstm inputs and outputs concatenated vertically \n",
    "        lstm_input_lst = []\n",
    "        lstm_output_lst = []\n",
    "        for i in range(len(sep_data)):\n",
    "            sep_values = sep_data[i].values\n",
    "            x_norm = x_scaler.transform(sep_values)\n",
    "            #[:,[0, 4]]\n",
    "            y_norm = y_scaler.transform(sep_values[:, [0]])\n",
    "            lstm_input_lst.append(self.create_LSTM_Input(x_norm, seq_length, 1))\n",
    "            lstm_output_lst.append(self.create_LSTM_Output(y_norm, seq_length))\n",
    "\n",
    "\n",
    "        lstm_input = np.concatenate(lstm_input_lst, axis = 0)\n",
    "        lstm_output = np.concatenate(lstm_output_lst, axis = 0)\n",
    "        \n",
    "        return lstm_input, lstm_output\n",
    "    \n",
    "    def preprocess_submission(self, combined_all):\n",
    "        self.test_data['구분'] = self.test_data['일자|시간|구분'].str[-1]\n",
    "        self.test_data['연월일'] = pd.to_datetime(self.test_data['일자|시간|구분'].str[:-5])\n",
    "        self.test_data['시간'] = self.test_data['일자|시간|구분'].str[-4:-2].astype(np.float32)\n",
    "        \n",
    "        self.test_data['year']  = self.test_data['연월일'].dt.year\n",
    "        self.test_data['month'] = self.test_data['연월일'].dt.month\n",
    "        self.test_data['day']   = self.test_data['연월일'].dt.day\n",
    "        \n",
    "        test_all = []\n",
    "        \n",
    "        for i in range(len(self.suppliers)):\n",
    "            test_sup = self.test_data.iloc[2160*i : 2160*(i+1)].reset_index(drop=True)\n",
    "            test_sup['공급량'] = -100\n",
    "            #print(test_sup['구분'].value_counts())\n",
    "            #print(combined_all[i].iloc[-3:][['year', 'month', 'day', '시간']])\n",
    "            #print(combined_all[i]['구분'].iloc[-1])\n",
    "            test_sup = test_sup[['구분', '공급량', 'year', 'month', 'day', '시간']]\n",
    "            \n",
    "            supplier = self.suppliers[i]\n",
    "            #past_data = combined_all[i].iloc[-3:][['구분', '공급량', 'year', 'month', 'day', '시간']].reset_index(drop=True)\n",
    "            past_data = self.get_past_data_submission(supplier)\n",
    "            \n",
    "            test_sup = pd.concat([past_data, test_sup], axis = 0).reset_index(drop=True)\n",
    "            print(test_sup.iloc[:5])\n",
    "            test_all.append(test_sup.iloc[:, 1:].values)\n",
    "        \n",
    "        return test_all\n",
    "    \n",
    "    def get_past_data_submission(self, supplier):\n",
    "        supplier = 'data_{}'.format(supplier)\n",
    "        past_data  = self.datas[supplier][ (self.datas[supplier]['month'] == 12) & \\\n",
    "                     (self.datas[supplier]['year'] == 2018) ][['구분', '공급량', 'year', 'month', 'day', '시간']].iloc[-3:]\n",
    "        return past_data\n",
    "    \n",
    "    def make_model(self):\n",
    "        input_x = Input(shape=(3,5)) #(sequence length, num of features) for LSTM; i.e. 50 minutes (sequence of 50 minutes)\n",
    "\n",
    "        x = LSTM(20, return_sequences=True)(input_x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('tanh')(x)\n",
    "\n",
    "        x = LSTM(10, return_sequences=True)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('tanh')(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = Dense(32)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        output = Dense(1)(x)\n",
    "\n",
    "\n",
    "        model = Model(inputs = input_x, outputs = output)\n",
    "        model.compile(loss=\"mean_absolute_error\", optimizer = Adam(lr=0.001)) \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_final_submission(self, test_all):\n",
    "        \n",
    "        for i in range(len(test_all)):\n",
    "            test_data = test_all[i]\n",
    "            model = self.make_model()\n",
    "            model.load_weights(self.model_paths[i])\n",
    "            \n",
    "            supplier = self.suppliers[i]\n",
    "            x_scaler_path = './Scalers/x_scaler_{}.pkl'.format(supplier)\n",
    "            y_scaler_path = './Scalers/y_scaler_{}.pkl'.format(supplier)\n",
    "            x_scaler = load(open(x_scaler_path, 'rb'))\n",
    "            y_scaler = load(open(y_scaler_path, 'rb'))\n",
    "            \n",
    "            for i in range(3, len(test_data)):\n",
    "                \n",
    "                input_x = test_data[i-3:i]\n",
    "                input_x = x_scaler.transform(input_x)\n",
    "                input_x = np.expand_dims(input_x, axis =0)\n",
    "\n",
    "                predicted = model.predict(input_x)\n",
    "                predicted = y_scaler.inverse_transform(predicted)[0][0]\n",
    "                \n",
    "                test_data[i, 0] = predicted\n",
    "                \n",
    "        \n",
    "        final_submission = np.concatenate(test_all, axis = 0)\n",
    "        \n",
    "        return final_submission\n",
    "    \n",
    "    def save_submission(self, final_submission):\n",
    "        \n",
    "        submission_all = []\n",
    "        for i in range(7):\n",
    "            submission_all.append( final_submission[i*2163+3 : (i+1)*2163] )\n",
    "\n",
    "\n",
    "        submission = np.concatenate(submission_all, axis = 0)\n",
    "        submission_supply = pd.DataFrame(submission[:,0])\n",
    "        submission_csv = pd.read_csv('./Submission/sample_submission.csv')\n",
    "        submission_csv.iloc[:,1] = submission_supply\n",
    "        submission_csv.to_csv('./Submission/submission.csv', index = False)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "d86ed589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/woojaebyun/Documents/Dacon_Gas_Prediction'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "db456a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Epoch_029_Val_0.014.hdf5', '.DS_Store']"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./Models/training_H/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "df79f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = data_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2e958d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.901489e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        1.000000e+00],\n",
       "       [1.706081e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        2.000000e+00],\n",
       "       [1.533921e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        3.000000e+00],\n",
       "       [1.611033e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        4.000000e+00],\n",
       "       [1.792161e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        5.000000e+00]])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_data = dp.get_december_data(dp.datas)\n",
    "jan_march_data = dp.get_jan_march_data(dp.datas)\n",
    "\n",
    "combined_all = dp.combine_data(dec_data, jan_march_data)\n",
    "\n",
    "combined_all[0][['공급량', 'year', 'month', 'day', '시간']].values[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "55f71744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2183, 4367, 6575, 8759, 10943]\n",
      "   Unnamed: 0         연월일  시간 구분      공급량  year  month  day\n",
      "0        8736  2013-12-31   1  H  448.692  2013     12   31\n",
      "1        8737  2013-12-31   2  H  383.316  2013     12   31\n",
      "2        8738  2013-12-31   3  H  321.844  2013     12   31\n",
      "3        8739  2013-12-31   4  H  334.348  2013     12   31\n",
      "4        8740  2013-12-31   5  H  398.196  2013     12   31\n",
      "[[0.05532686 0.         1.         1.         0.        ]\n",
      " [0.04535946 0.         1.         1.         0.04347826]\n",
      " [0.03598728 0.         1.         1.         0.08695652]]\n",
      "(10929, 1)\n",
      "    Unnamed: 0         연월일  시간 구분       공급량  year  month  day\n",
      "20        8756  2013-12-31  21  A  2185.665  2013     12   31\n",
      "21        8757  2013-12-31  22  A  2097.929  2013     12   31\n",
      "22        8758  2013-12-31  23  A  1999.705  2013     12   31\n",
      "23        8759  2013-12-31  24  A  1858.169  2013     12   31\n",
      "24        8760  2014-01-01   1  A  1677.257  2014      1    1\n"
     ]
    }
   ],
   "source": [
    "sep_data = dp.separate_data(combined_all[6])\n",
    "print(combined_all[6].iloc[:5])\n",
    "\n",
    "x_scaler = load(open('./Scalers/x_scaler_H.pkl', 'rb'))\n",
    "y_scaler = load(open('./Scalers/y_scaler_H.pkl', 'rb'))\n",
    "lstm_input, lstm_output = dp.separate_data_to_lstm_input(sep_data, x_scaler, y_scaler, 3)\n",
    "\n",
    "print(lstm_input[0])\n",
    "print(lstm_output.shape)\n",
    "\n",
    "\n",
    "print(combined_all[0].iloc[20:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "aa5d9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  구분       공급량  year  month  day    시간\n",
      "0  A  2840.272  2018     12   31  22.0\n",
      "1  A  2692.385  2018     12   31  23.0\n",
      "2  A  2583.340  2018     12   31  24.0\n",
      "3  A  -100.000  2019      1    1   1.0\n",
      "4  A  -100.000  2019      1    1   2.0\n",
      "  구분       공급량  year  month  day    시간\n",
      "0  B  2540.169  2018     12   31  22.0\n",
      "1  B  2407.945  2018     12   31  23.0\n",
      "2  B  2290.154  2018     12   31  24.0\n",
      "3  B  -100.000  2019      1    1   1.0\n",
      "4  B  -100.000  2019      1    1   2.0\n",
      "  구분      공급량  year  month  day    시간\n",
      "0  C  256.000  2018     12   31  22.0\n",
      "1  C  246.020  2018     12   31  23.0\n",
      "2  C  237.911  2018     12   31  24.0\n",
      "3  C -100.000  2019      1    1   1.0\n",
      "4  C -100.000  2019      1    1   2.0\n",
      "  구분       공급량  year  month  day    시간\n",
      "0  D  1720.595  2018     12   31  22.0\n",
      "1  D  1624.138  2018     12   31  23.0\n",
      "2  D  1422.478  2018     12   31  24.0\n",
      "3  D  -100.000  2019      1    1   1.0\n",
      "4  D  -100.000  2019      1    1   2.0\n",
      "  구분       공급량  year  month  day    시간\n",
      "0  E  3954.210  2018     12   31  22.0\n",
      "1  E  3745.844  2018     12   31  23.0\n",
      "2  E  3534.260  2018     12   31  24.0\n",
      "3  E  -100.000  2019      1    1   1.0\n",
      "4  E  -100.000  2019      1    1   2.0\n",
      "  구분       공급량  year  month  day    시간\n",
      "0  G  4354.915  2018     12   31  22.0\n",
      "1  G  4204.484  2018     12   31  23.0\n",
      "2  G  3982.757  2018     12   31  24.0\n",
      "3  G  -100.000  2019      1    1   1.0\n",
      "4  G  -100.000  2019      1    1   2.0\n",
      "  구분      공급량  year  month  day    시간\n",
      "0  H  657.941  2018     12   31  22.0\n",
      "1  H  610.953  2018     12   31  23.0\n",
      "2  H  560.896  2018     12   31  24.0\n",
      "3  H -100.000  2019      1    1   1.0\n",
      "4  H -100.000  2019      1    1   2.0\n",
      "[[2840.272 2018.      12.      31.      22.   ]\n",
      " [2692.385 2018.      12.      31.      23.   ]\n",
      " [2583.34  2018.      12.      31.      24.   ]]\n",
      "[[2540.169 2018.      12.      31.      22.   ]\n",
      " [2407.945 2018.      12.      31.      23.   ]\n",
      " [2290.154 2018.      12.      31.      24.   ]]\n",
      "[[ 256.    2018.      12.      31.      22.   ]\n",
      " [ 246.02  2018.      12.      31.      23.   ]\n",
      " [ 237.911 2018.      12.      31.      24.   ]]\n",
      "[[1720.595 2018.      12.      31.      22.   ]\n",
      " [1624.138 2018.      12.      31.      23.   ]\n",
      " [1422.478 2018.      12.      31.      24.   ]]\n",
      "[[3954.21  2018.      12.      31.      22.   ]\n",
      " [3745.844 2018.      12.      31.      23.   ]\n",
      " [3534.26  2018.      12.      31.      24.   ]]\n",
      "[[4354.915 2018.      12.      31.      22.   ]\n",
      " [4204.484 2018.      12.      31.      23.   ]\n",
      " [3982.757 2018.      12.      31.      24.   ]]\n",
      "[[ 657.941 2018.      12.      31.      22.   ]\n",
      " [ 610.953 2018.      12.      31.      23.   ]\n",
      " [ 560.896 2018.      12.      31.      24.   ]]\n"
     ]
    }
   ],
   "source": [
    "test_all = dp.preprocess_submission(combined_all)\n",
    "for test in test_all:\n",
    "    print(test[:3])\n",
    "#past_data = dp.get_past_data_submission('A')\n",
    "#past_data\n",
    "#dp.datas['data_A'].iloc[-3:]\n",
    "#for test_sup in test_all:\n",
    "    #print(test_sup[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "9db4b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-23 22:01:12.768925: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:01:12.827769: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:01:12.855073: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-23 22:01:56.013893: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:01:56.068684: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:01:56.092107: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-23 22:02:38.806431: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:02:38.860677: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:02:38.882522: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-23 22:03:23.156580: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:03:23.216230: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:03:23.237403: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-23 22:04:07.900970: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:04:07.962475: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:04:07.985761: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-23 22:04:51.371322: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:04:51.427836: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:04:51.448238: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-10-23 22:05:35.219763: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:05:35.276384: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-23 22:05:35.297705: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "final_submission = dp.get_final_submission(test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "6a2a7c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15141, 5)\n",
      "(2160, 5)\n",
      "[[1.84255078e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [2.12688135e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  2.00000000e+00]\n",
      " [1.67041846e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  3.00000000e+00]]\n",
      "(2160, 5)\n",
      "[[2.02685742e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [1.42830042e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  2.00000000e+00]\n",
      " [1.51032361e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  3.00000000e+00]]\n",
      "(2160, 5)\n",
      "[[1.90097061e+02 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [1.85554565e+02 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  2.00000000e+00]\n",
      " [2.28014725e+02 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  3.00000000e+00]]\n",
      "(2160, 5)\n",
      "[[1.06797571e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [9.58042786e+02 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  2.00000000e+00]\n",
      " [9.35798157e+02 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  3.00000000e+00]]\n",
      "(2160, 5)\n",
      "[[2.32580127e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [1.39995288e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  2.00000000e+00]\n",
      " [7.81955994e+02 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  3.00000000e+00]]\n",
      "(2160, 5)\n",
      "[[3.17237061e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [3.14565967e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  2.00000000e+00]\n",
      " [2.21259888e+03 2.01900000e+03 1.00000000e+00 1.00000000e+00\n",
      "  3.00000000e+00]]\n",
      "(2160, 5)\n",
      "[[ 4.74716125e+02  2.01900000e+03  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00]\n",
      " [ 3.66002007e+01  2.01900000e+03  1.00000000e+00  1.00000000e+00\n",
      "   2.00000000e+00]\n",
      " [-3.93597527e+01  2.01900000e+03  1.00000000e+00  1.00000000e+00\n",
      "   3.00000000e+00]]\n",
      "Index(['일자|시간|구분', '공급량'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['일자|시간|구분', '공급량'], dtype='object')"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_submission.shape)\n",
    "\n",
    "submission_all = []\n",
    "for i in range(7):\n",
    "    submission_all.append( final_submission[i*2163+3 : (i+1)*2163] )\n",
    "\n",
    "\n",
    "submission = np.concatenate(submission_all, axis = 0)\n",
    "submission_supply = pd.DataFrame(submission[:,0])\n",
    "submission_csv = pd.read_csv('./Submission/sample_submission.csv')\n",
    "submission_csv.iloc[:,1] = submission_supply\n",
    "submission_csv.to_csv('./Submission/submission.csv', index = False)\n",
    "\n",
    "    \n",
    "for submission in submission_all:\n",
    "    print(submission.shape)\n",
    "    print(submission[:3])\n",
    "\n",
    "submission_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8259d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2840.272 2018.      12.      31.      22.   ]\n",
      " [2692.385 2018.      12.      31.      23.   ]\n",
      " [2583.34  2018.      12.      31.      24.   ]]\n",
      "[[0.65064148 1.         1.         1.         0.91304348]\n",
      " [0.61088068 1.         1.         1.         0.95652174]\n",
      " [0.58156292 1.         1.         1.         1.        ]]\n",
      "[[[0.65064148 1.         1.         1.         0.91304348]\n",
      "  [0.61088068 1.         1.         1.         0.95652174]\n",
      "  [0.58156292 1.         1.         1.         1.        ]]]\n",
      "[[0.38239482]]\n",
      "4139.682\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"./Models/training_A/Epoch_027_Val_0.055.hdf5\")\n",
    "x_scaler = load(open('./Scalers/x_scaler_A.pkl', 'rb'))\n",
    "y_scaler = load(open('./Scalers/y_scaler_A.pkl', 'rb'))\n",
    "\n",
    "input_x = test_all[0][:3]\n",
    "print(input_x)\n",
    "input_x = x_scaler.transform(input_x)\n",
    "print(input_x)\n",
    "input_x = np.expand_dims(input_x, axis =0)\n",
    "print(input_x)\n",
    "input_x.shape\n",
    "\n",
    "predicted = model.predict(input_x)\n",
    "print(predicted)\n",
    "predicted = y_scaler.inverse_transform(predicted)[0][0]\n",
    "predicted\n",
    "\n",
    "print(y_scaler.inverse_transform([[1]])[0][0])\n",
    "\n",
    "#test_all[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f89a4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(lstm_input, lstm_output, test_size = 0.2, random_state = 1311, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3a5df9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, Dense, Input, LSTM, Flatten, Concatenate, Bidirectional, BatchNormalization, Dropout, ReLU, Activation, ConvLSTM2D, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "c9a09bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 3, 5)]            0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 3, 20)             2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 3, 20)             80        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 3, 20)             0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 3, 10)             1240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 3, 10)             40        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 3, 10)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                992       \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,593\n",
      "Trainable params: 4,469\n",
      "Non-trainable params: 124\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_x = Input(shape=(3,5)) #(sequence length, num of features) for LSTM; i.e. 50 minutes (sequence of 50 minutes)\n",
    "\n",
    "x = LSTM(20, return_sequences=True)(input_x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "x = LSTM(10, return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "output = Dense(1)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs = input_x, outputs = output)\n",
    "model.compile(loss=\"mean_absolute_error\", optimizer = Adam(lr=0.001)) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4ccefa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:41:28.735639: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/69 [..............................] - ETA: 1:33 - loss: 0.3836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:41:28.987776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:29.021230: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:29.076823: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:29.122809: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - ETA: 0s - loss: 0.1626"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:41:31.168805: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:31.233445: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:31.255792: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 4s 33ms/step - loss: 0.1626 - val_loss: 0.0439\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04386, saving model to ./Models/training_H/Epoch_001_Val_0.044.hdf5\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 2s 27ms/step - loss: 0.0794 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04386\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 2s 27ms/step - loss: 0.0609 - val_loss: 0.1067\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04386\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0492 - val_loss: 0.1041\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04386\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0434 - val_loss: 0.1097\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04386\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0390 - val_loss: 0.1048\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04386\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0360 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04386\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0319 - val_loss: 0.0483\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04386\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0293 - val_loss: 0.0366\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04386 to 0.03660, saving model to ./Models/training_H/Epoch_009_Val_0.037.hdf5\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0258 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03660\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0243 - val_loss: 0.0269\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03660 to 0.02695, saving model to ./Models/training_H/Epoch_011_Val_0.027.hdf5\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0235 - val_loss: 0.0271\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02695\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0228 - val_loss: 0.0320\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02695\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0208 - val_loss: 0.0252\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02695 to 0.02520, saving model to ./Models/training_H/Epoch_014_Val_0.025.hdf5\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0211 - val_loss: 0.0226\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02520 to 0.02258, saving model to ./Models/training_H/Epoch_015_Val_0.023.hdf5\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0221 - val_loss: 0.0619\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02258\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0281 - val_loss: 0.0537\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02258\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0234 - val_loss: 0.0391\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02258\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0234 - val_loss: 0.0413\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02258\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0221 - val_loss: 0.0283\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02258\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0195 - val_loss: 0.0290\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02258\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0183 - val_loss: 0.0230\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02258\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0174 - val_loss: 0.0219\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02258 to 0.02186, saving model to ./Models/training_H/Epoch_023_Val_0.022.hdf5\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0167 - val_loss: 0.0322\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02186\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0161 - val_loss: 0.0241\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02186\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0153 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02186 to 0.01937, saving model to ./Models/training_H/Epoch_026_Val_0.019.hdf5\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0154 - val_loss: 0.0141\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01937 to 0.01406, saving model to ./Models/training_H/Epoch_027_Val_0.014.hdf5\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0149 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01406\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0146 - val_loss: 0.0139\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01406 to 0.01389, saving model to ./Models/training_H/Epoch_029_Val_0.014.hdf5\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01389\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode = 'min' , patience = 10, verbose = 1)\n",
    "\n",
    "path = './Models/training_{}'.format('H')\n",
    "if os.path.isdir(path):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "os.makedirs(path, exist_ok = True)\n",
    "\n",
    "file_path = path + '/Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5'\n",
    "mc = ModelCheckpoint(file_path, monitor='val_loss', mode='min',verbose=1, \\\n",
    "                     save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit(train_x, train_y, batch_size = 128, epochs =30, validation_data = (valid_x, valid_y), callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "794ec892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2c04d97c0>]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6559a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
