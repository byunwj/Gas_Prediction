{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6e14af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pickle import dump, load\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "268e491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.years     = [2013, 2014, 2015, 2016, 2017, 2018]\n",
    "        self.suppliers = ['A', 'B', 'C', 'D', 'E', 'G', 'H']\n",
    "        self.datas     = self.load_data()\n",
    "        self.month_nums  = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        \n",
    "    \n",
    "    def load_data(self):\n",
    "        datas = dict()\n",
    "        for supplier in suppliers:\n",
    "            datas['data_{}'.format(supplier)] = pd.read_csv('./Data/supplier_{}.csv'.format(supplier)) \n",
    "        \n",
    "        return datas\n",
    "    \n",
    "    def get_december_data(self, datas):\n",
    "        # getting the last day of december (31st) of each year except 2018\n",
    "        \n",
    "        training_12_all = []\n",
    "        for key in datas.keys():\n",
    "            training_12  = datas[key][ (datas[key]['month'] == 12) & \\\n",
    "                                      (datas[key]['year'].isin([2013, 2014, 2015,2016, 2017])) ].reset_index(drop = True)\n",
    "\n",
    "\n",
    "            ### for cutting out all december days except the 31st\n",
    "            training_12 = training_12.sort_values(['year', 'month','day']).reset_index(drop=True)\n",
    "\n",
    "            #print(training_12['year'].value_counts())\n",
    "            training_12_cut = []\n",
    "            for i in range(1,6):\n",
    "                training_12_cut.append(training_12.iloc[(744*i)-24:744*i])\n",
    "\n",
    "            training_12_final = pd.concat(training_12_cut, axis = 0).reset_index(drop=True)\n",
    "            #print(training_12_final['year'].value_counts())\n",
    "            \n",
    "            training_12_all.append(training_12_final)\n",
    "            \n",
    "        \n",
    "        return training_12_all\n",
    "    \n",
    "    def get_jan_march_data(self, datas):\n",
    "        \n",
    "        training_1_3_all = []\n",
    "        \n",
    "        for key in datas.keys():  \n",
    "            training_1_3 = datas[key][ (datas[key]['month'].isin([1,2,3])) & \\\n",
    "                             (datas[key]['year'].isin([2014, 2015, 2016, 2017, 2018])) ].reset_index(drop = True)\n",
    "            \n",
    "            training_1_3_all.append(training_1_3)\n",
    "        \n",
    "        return training_1_3_all\n",
    "    \n",
    "    def combine_data(self, training_12_all, training_1_3_all):\n",
    "        \n",
    "        combined_all = []\n",
    "        \n",
    "        assert len(training_12_all) == len(training_1_3_all), \"december and jan-march data lengths should match\"\n",
    "        \n",
    "        for i in range(len(training_12_all)):\n",
    "            training = pd.concat([training_12_all[i], training_1_3_all[i]], axis = 0).reset_index(drop=True)\n",
    "            training = training.sort_values(['year', 'month']).reset_index(drop=True)\n",
    "            \n",
    "            combined_all.append(training)\n",
    "        \n",
    "        return combined_all\n",
    "    \n",
    "    def separate_data(self, combined_data):\n",
    "        # separate 12/31 - 3/31 for each pair of year (e.g. 2013-2014, 2014-2015, etc.)\n",
    "        indices = []\n",
    "        for year in self.years[1:]: # march of 2014-2018\n",
    "            indices.append( combined_data[ (combined_data['year'] == year) & (combined_data['month'] == 3) &\\\n",
    "                           (combined_data['day'] == 31) & (combined_data['시간'] == 24)].index[0] )\n",
    "        print(indices)\n",
    "        training_sep = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            idx = idx + 1 \n",
    "            #if idx != training_A.shape[0] -1 else idx\n",
    "            training_sep.append(combined_data[last_idx:idx][['공급량', 'year', 'month', 'day', '시간']])\n",
    "            last_idx = idx\n",
    "        \n",
    "        return training_sep\n",
    "    \n",
    "    \n",
    "    def make_scalers(self, combined_all):\n",
    "        combined = combined_all[:]\n",
    "        # change pandas dataframe to numpy ndarray\n",
    "        for i in range(len(combined_all)): # loop through suppliers \n",
    "            supplier = combined[i]['구분'].iloc[0]\n",
    "            assert supplier == self.suppliers[i], 'suppliers should match'\n",
    "            combined[i] = combined[i][['공급량', 'year', 'month', 'day', '시간']].values\n",
    "            #print(combined[i].shape)\n",
    "            x_scaler = MinMaxScaler()\n",
    "            y_scaler = MinMaxScaler()\n",
    "            x_scaler.fit(combined[i])\n",
    "            y_scaler.fit(combined[i][:, [0]])\n",
    "            \n",
    "            path_x = './Scalers/x_scaler_{}.pkl'.format(supplier)\n",
    "            dump(x_scaler, open(path_x, 'wb'))\n",
    "            path_y = './Scalers/y_scaler_{}.pkl'.format(supplier)\n",
    "            dump(y_scaler, open(path_y, 'wb'))\n",
    "            \n",
    "        \n",
    "    \n",
    "    def create_LSTM_Input(self, data, seq_length, step_size):\n",
    "        final_data = np.zeros((data.shape[0]-seq_length, int(seq_length/step_size), data.shape[1]))\n",
    "\n",
    "        length = data.shape[0]\n",
    "\n",
    "        for i in range(final_data.shape[0]):\n",
    "            final_data[i] = data[i:i+seq_length:step_size]\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    def create_LSTM_Output(self, data, seq_length):\n",
    "        final_output = []\n",
    "        length = data.shape[0]\n",
    "\n",
    "        for i in range(seq_length, length):\n",
    "            final_output.append(data[i])\n",
    "\n",
    "        final_output = np.array(final_output)\n",
    "        #final_output = np.expand_dims(final_output, axis = 1)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    \n",
    "    def separate_data_to_lstm_input(self, sep_data, x_scaler, y_scaler, seq_length): \n",
    "        # need to make each chunck 12-3 into lstm inputs SEPARATELY\n",
    "        lstm_input_lst = []\n",
    "        lstm_output_lst = []\n",
    "        for i in range(len(sep_data)):\n",
    "            sep_values = sep_data[i].values\n",
    "            x_norm = x_scaler.transform(sep_values)\n",
    "            #[:,[0, 4]]\n",
    "            y_norm = y_scaler.transform(sep_values[:, [0]])\n",
    "            lstm_input_lst.append(self.create_LSTM_Input(x_norm, seq_length, 1))\n",
    "            lstm_output_lst.append(self.create_LSTM_Output(y_norm, seq_length))\n",
    "\n",
    "\n",
    "        lstm_input = np.concatenate(lstm_input_lst, axis = 0)\n",
    "        lstm_output = np.concatenate(lstm_output_lst, axis = 0)\n",
    "        \n",
    "        return lstm_input, lstm_output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fe46ee9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/woojaebyun/Documents/Dacon_Gas_Prediction'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c966e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "950b2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = data_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b180a74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.901489e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        1.000000e+00],\n",
       "       [1.706081e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        2.000000e+00],\n",
       "       [1.533921e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        3.000000e+00],\n",
       "       [1.611033e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        4.000000e+00],\n",
       "       [1.792161e+03, 2.013000e+03, 1.200000e+01, 3.100000e+01,\n",
       "        5.000000e+00]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_data = dp.get_december_data(dp.datas)\n",
    "jan_march_data = dp.get_jan_march_data(dp.datas)\n",
    "\n",
    "combined_all = dp.combine_data(dec_data, jan_march_data)\n",
    "\n",
    "combined_all[0][['공급량', 'year', 'month', 'day', '시간']].values[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "70faf21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2183, 4367, 6575, 8759, 10943]\n",
      "   Unnamed: 0         연월일  시간 구분      공급량  year  month  day\n",
      "0        8736  2013-12-31   1  H  448.692  2013     12   31\n",
      "1        8737  2013-12-31   2  H  383.316  2013     12   31\n",
      "2        8738  2013-12-31   3  H  321.844  2013     12   31\n",
      "3        8739  2013-12-31   4  H  334.348  2013     12   31\n",
      "4        8740  2013-12-31   5  H  398.196  2013     12   31\n",
      "(10929, 3, 5)\n",
      "(10929, 1)\n"
     ]
    }
   ],
   "source": [
    "sep_data = dp.separate_data(combined_all[6])\n",
    "print(combined_all[6].iloc[:5])\n",
    "\n",
    "x_scaler = load(open('./Scalers/x_scaler_H.pkl', 'rb'))\n",
    "y_scaler = load(open('./Scalers/y_scaler_H.pkl', 'rb'))\n",
    "lstm_input, lstm_output = dp.separate_data_to_lstm_input(sep_data, x_scaler, y_scaler, 3)\n",
    "\n",
    "print(lstm_input.shape)\n",
    "print(lstm_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ca519229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(lstm_input, lstm_output, test_size = 0.2, random_state = 1311, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e2851d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, Dense, Input, LSTM, Flatten, Concatenate, Bidirectional, BatchNormalization, Dropout, ReLU, Activation, ConvLSTM2D, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "72260c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 3, 5)]            0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 3, 20)             2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 3, 20)             80        \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 3, 20)             0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 3, 10)             1240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 3, 10)             40        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 3, 10)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                992       \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,593\n",
      "Trainable params: 4,469\n",
      "Non-trainable params: 124\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/woojaebyun/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_x = Input(shape=(3,5)) #(sequence length, num of features) for LSTM; i.e. 50 minutes (sequence of 50 minutes)\n",
    "\n",
    "x = LSTM(20, return_sequences=True)(input_x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "x = LSTM(10, return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "output = Dense(1)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs = input_x, outputs = output)\n",
    "model.compile(loss=\"mean_absolute_error\", optimizer = Adam(lr=0.001)) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5901a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:41:28.735639: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/69 [..............................] - ETA: 1:33 - loss: 0.3836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:41:28.987776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:29.021230: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:29.076823: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:29.122809: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - ETA: 0s - loss: 0.1626"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 22:41:31.168805: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:31.233445: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-21 22:41:31.255792: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 4s 33ms/step - loss: 0.1626 - val_loss: 0.0439\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04386, saving model to ./Models/training_H/Epoch_001_Val_0.044.hdf5\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 2s 27ms/step - loss: 0.0794 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04386\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 2s 27ms/step - loss: 0.0609 - val_loss: 0.1067\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04386\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0492 - val_loss: 0.1041\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04386\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0434 - val_loss: 0.1097\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04386\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0390 - val_loss: 0.1048\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04386\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0360 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04386\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0319 - val_loss: 0.0483\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04386\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0293 - val_loss: 0.0366\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04386 to 0.03660, saving model to ./Models/training_H/Epoch_009_Val_0.037.hdf5\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0258 - val_loss: 0.0425\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03660\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0243 - val_loss: 0.0269\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03660 to 0.02695, saving model to ./Models/training_H/Epoch_011_Val_0.027.hdf5\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0235 - val_loss: 0.0271\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02695\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0228 - val_loss: 0.0320\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02695\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0208 - val_loss: 0.0252\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02695 to 0.02520, saving model to ./Models/training_H/Epoch_014_Val_0.025.hdf5\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0211 - val_loss: 0.0226\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02520 to 0.02258, saving model to ./Models/training_H/Epoch_015_Val_0.023.hdf5\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0221 - val_loss: 0.0619\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02258\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0281 - val_loss: 0.0537\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02258\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0234 - val_loss: 0.0391\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02258\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0234 - val_loss: 0.0413\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02258\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0221 - val_loss: 0.0283\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02258\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0195 - val_loss: 0.0290\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02258\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0183 - val_loss: 0.0230\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02258\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0174 - val_loss: 0.0219\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02258 to 0.02186, saving model to ./Models/training_H/Epoch_023_Val_0.022.hdf5\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0167 - val_loss: 0.0322\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02186\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0161 - val_loss: 0.0241\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02186\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0153 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02186 to 0.01937, saving model to ./Models/training_H/Epoch_026_Val_0.019.hdf5\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0154 - val_loss: 0.0141\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01937 to 0.01406, saving model to ./Models/training_H/Epoch_027_Val_0.014.hdf5\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0149 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01406\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0146 - val_loss: 0.0139\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01406 to 0.01389, saving model to ./Models/training_H/Epoch_029_Val_0.014.hdf5\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01389\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode = 'min' , patience = 10, verbose = 1)\n",
    "\n",
    "path = './Models/training_{}'.format('H')\n",
    "if os.path.isdir(path):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "os.makedirs(path, exist_ok = True)\n",
    "\n",
    "file_path = path + '/Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5'\n",
    "mc = ModelCheckpoint(file_path, monitor='val_loss', mode='min',verbose=1, \\\n",
    "                     save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit(train_x, train_y, batch_size = 128, epochs =30, validation_data = (valid_x, valid_y), callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c9e0d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2c04d97c0>]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8151bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
